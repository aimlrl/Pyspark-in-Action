{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession is an entry point to PySpark's functionality within a program. This entry point lets you access the features of Spark through Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"Converting articles into BoW Vectors\").getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-MLVSQCI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Converting articles into BoW Vectors</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ef65616ac0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"all-the-news-2-1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The older version of PySpark used to store the data in special kind of Data Structure called Resilient Distributed Dataset (RDD). The special feature of this data structure is that it is Row Major. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The new version of PySpark has also introduced a column major data structure called PySpark DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+----------------+-----+----+-----------+--------------------+--------------------+--------------------+--------------------+------------+\n",
      "|                 _c0|                 _c1|                _c2|             _c3|  _c4| _c5|        _c6|                 _c7|                 _c8|                 _c9|                _c10|        _c11|\n",
      "+--------------------+--------------------+-------------------+----------------+-----+----+-----------+--------------------+--------------------+--------------------+--------------------+------------+\n",
      "|                null|          Unnamed: 0|               date|            year|month| day|     author|               title|             article|                 url|             section| publication|\n",
      "|                   0|                   0|2016-12-09 18:31:00|            2016| 12.0|   9|Lee Drutman|We should take co...|\"This post is par...|             however| several critics ...| for example|\n",
      "|                   1|                   1|2016-10-07 21:26:46|            2016| 10.0|   7|Scott Davis|Colts GM Ryan Gri...| The Indianapolis...|                null|                null|        null|\n",
      "|The highest-paid ...|https://www.busin...|               null|Business Insider| null|null|       null|                null|                null|                null|                null|        null|\n",
      "|                   2|                   2|2018-01-26 00:00:00|            2018|  1.0|  26|       null|Trump denies repo...|DAVOS, Switzerlan...|https://www.reute...|               Davos|     Reuters|\n",
      "+--------------------+--------------------+-------------------+----------------+-----+----+-----------+--------------------+--------------------+--------------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string'), ('_c4', 'string'), ('_c5', 'string'), ('_c6', 'string'), ('_c7', 'string'), ('_c8', 'string'), ('_c9', 'string'), ('_c10', 'string'), ('_c11', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+----------------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 _c0|                 _c1|                _c2|             _c3|  _c4| _c5|                 _c6|                 _c7|                 _c8|                 _c9|                _c10|                _c11|\n",
      "+--------------------+--------------------+-------------------+----------------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                null|          Unnamed: 0|               date|            year|month| day|              author|               title|             article|                 url|             section|         publication|\n",
      "|                   0|                   0|2016-12-09 18:31:00|            2016| 12.0|   9|         Lee Drutman|We should take co...|\"This post is par...|             however| several critics ...|         for example|\n",
      "|                   1|                   1|2016-10-07 21:26:46|            2016| 10.0|   7|         Scott Davis|Colts GM Ryan Gri...| The Indianapolis...|                null|                null|                null|\n",
      "|The highest-paid ...|https://www.busin...|               null|Business Insider| null|null|                null|                null|                null|                null|                null|                null|\n",
      "|                   2|                   2|2018-01-26 00:00:00|            2018|  1.0|  26|                null|Trump denies repo...|DAVOS, Switzerlan...|https://www.reute...|               Davos|             Reuters|\n",
      "|                   3|                   3|2019-06-27 00:00:00|            2019|  6.0|  27|                null|France's Sarkozy ...|PARIS (Reuters) -...|https://www.reute...|          World News|             Reuters|\n",
      "|                   4|                   4|2016-01-27 00:00:00|            2016|  1.0|  27|                null|Paris Hilton: Wom...|\"Paris Hilton arr...|https://www.tmz.c...|                null|                 TMZ|\n",
      "|                   5|                   5|2019-06-17 00:00:00|            2019|  6.0|  17|                null|ECB's Coeure: If ...|BERLIN, June 17 (...|                null|                null|                null|\n",
      "|Editing by Tassil...|https://www.reute...|         Financials|         Reuters| null|null|                null|                null|                null|                null|                null|                null|\n",
      "|                   6|                   6|2019-06-23 00:00:00|            2019|  6.0|  23|                null|Venezuela detains...|CARACAS (Reuters)...|https://www.reute...|          World News|             Reuters|\n",
      "|                   7|                   7|2018-05-02 17:09:00|            2018|  5.0|   2|   Caroline Williams|You Can Trick You...|\"If only every da...|\"\" DeGutis says. ...| paying attention...| it makes it easi...|\n",
      "|                   8|                   8|2016-05-18 13:00:06|            2016|  5.0|  18|         Mark Bergen|How to watch the ...|Google I/O, the c...|https://www.vox.c...|                null|                 Vox|\n",
      "|                   9|                   9|2017-03-02 00:00:00|            2017|  3.0|   2|            Tim Hume|China is dismissi...|China is dismissi...|https://news.vice...|                null|           Vice News|\n",
      "|                  10|                  10|2019-05-22 20:10:00|            2019|  5.0|  22|       Emily Stewart|“Elizabeth Warren...|Elizabeth Warren ...|https://www.vox.c...|                null|                 Vox|\n",
      "|                  11|                  11|2019-06-23 00:00:00|            2019|  6.0|  23|Jessica DiNapoli,...|Hudson's Bay's ch...|(Reuters) - The s...|https://www.reute...|       Business News|             Reuters|\n",
      "|                  12|                  12|2018-11-05 00:00:00|            2018| 11.0|   5|                null|Joakim Noah's Vic...|Joakim Noah's ﻿mo...|https://www.tmz.c...|                null|                 TMZ|\n",
      "|                  13|                  13|2019-05-10 00:00:00|            2019|  5.0|  10|                null|Jermaine Jackson ...|\"Jermaine Jackson...|\"\" \"\"Thriller\"\" a...| and the 2 will f...| Quincy isn't say...|\n",
      "|                  14|                  14|2019-03-28 00:00:00|            2019|  3.0|  28|                null|UK PM May presses...|LONDON (Reuters) ...|https://www.reute...|          World News|             Reuters|\n",
      "|                  15|                  15|2016-09-08 15:40:02|            2016|  9.0|   8|          Jeff Stein|Nancy Pelosi says...|\"Nancy Pelosi is ...|\"\" Skelley says. ...|           of course| but it begins lo...|\n",
      "|                  16|                  16|2017-01-11 23:10:01|            2017|  1.0|  11|        Libby Nelson|The government of...|The nonpartisan d...|https://www.vox.c...|                null|                 Vox|\n",
      "+--------------------+--------------------+-------------------+----------------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = data.select(\"_c8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c8|\n",
      "+--------------------+\n",
      "|             article|\n",
      "|\"This post is par...|\n",
      "| The Indianapolis...|\n",
      "|                null|\n",
      "|DAVOS, Switzerlan...|\n",
      "|PARIS (Reuters) -...|\n",
      "|\"Paris Hilton arr...|\n",
      "|BERLIN, June 17 (...|\n",
      "|                null|\n",
      "|CARACAS (Reuters)...|\n",
      "|\"If only every da...|\n",
      "|Google I/O, the c...|\n",
      "|China is dismissi...|\n",
      "|Elizabeth Warren ...|\n",
      "|(Reuters) - The s...|\n",
      "|Joakim Noah's ﻿mo...|\n",
      "|\"Jermaine Jackson...|\n",
      "|LONDON (Reuters) ...|\n",
      "|\"Nancy Pelosi is ...|\n",
      "|The nonpartisan d...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = data.select(data[\"_c8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c8|\n",
      "+--------------------+\n",
      "|             article|\n",
      "|\"This post is par...|\n",
      "| The Indianapolis...|\n",
      "|                null|\n",
      "|DAVOS, Switzerlan...|\n",
      "|PARIS (Reuters) -...|\n",
      "|\"Paris Hilton arr...|\n",
      "|BERLIN, June 17 (...|\n",
      "|                null|\n",
      "|CARACAS (Reuters)...|\n",
      "|\"If only every da...|\n",
      "|Google I/O, the c...|\n",
      "|China is dismissi...|\n",
      "|Elizabeth Warren ...|\n",
      "|(Reuters) - The s...|\n",
      "|Joakim Noah's ﻿mo...|\n",
      "|\"Jermaine Jackson...|\n",
      "|LONDON (Reuters) ...|\n",
      "|\"Nancy Pelosi is ...|\n",
      "|The nonpartisan d...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = data.select(data._c8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c8|\n",
      "+--------------------+\n",
      "|             article|\n",
      "|\"This post is par...|\n",
      "| The Indianapolis...|\n",
      "|                null|\n",
      "|DAVOS, Switzerlan...|\n",
      "|PARIS (Reuters) -...|\n",
      "|\"Paris Hilton arr...|\n",
      "|BERLIN, June 17 (...|\n",
      "|                null|\n",
      "|CARACAS (Reuters)...|\n",
      "|\"If only every da...|\n",
      "|Google I/O, the c...|\n",
      "|China is dismissi...|\n",
      "|Elizabeth Warren ...|\n",
      "|(Reuters) - The s...|\n",
      "|Joakim Noah's ﻿mo...|\n",
      "|\"Jermaine Jackson...|\n",
      "|LONDON (Reuters) ...|\n",
      "|\"Nancy Pelosi is ...|\n",
      "|The nonpartisan d...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = data.select(col(\"_c8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c8|\n",
      "+--------------------+\n",
      "|             article|\n",
      "|\"This post is par...|\n",
      "| The Indianapolis...|\n",
      "|                null|\n",
      "|DAVOS, Switzerlan...|\n",
      "|PARIS (Reuters) -...|\n",
      "|\"Paris Hilton arr...|\n",
      "|BERLIN, June 17 (...|\n",
      "|                null|\n",
      "|CARACAS (Reuters)...|\n",
      "|\"If only every da...|\n",
      "|Google I/O, the c...|\n",
      "|China is dismissi...|\n",
      "|Elizabeth Warren ...|\n",
      "|(Reuters) - The s...|\n",
      "|Joakim Noah's ﻿mo...|\n",
      "|\"Jermaine Jackson...|\n",
      "|LONDON (Reuters) ...|\n",
      "|\"Nancy Pelosi is ...|\n",
      "|The nonpartisan d...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = data.select(split(col(\"_c8\"),\" \").alias(\"article\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                             article|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                           [article]|\n",
      "|[\"This, post, is, part, of, Polyarchy,, an, independent, blog, produced, by, the, political, refo...|\n",
      "|[, The, Indianapolis, Colts, made, Andrew, Luck, the, highest-paid, player, in, NFL, history, thi...|\n",
      "|                                                                                                null|\n",
      "|[DAVOS,, Switzerland, (Reuters), -, U.S., President, Donald, Trump, denied, a, report, on, Friday...|\n",
      "|[PARIS, (Reuters), -, Former, French, president, Nicolas, Sarkozy, published, a, new, memoir, on,...|\n",
      "|[\"Paris, Hilton, arrived, at, LAX, Wednesday, dressed, to, pay, her, last, respects, to, her, unc...|\n",
      "|[BERLIN,, June, 17, (Reuters), -, ECB, board, member, Benoit, Coeure, said, in, an, interview, pu...|\n",
      "|                                                                                                null|\n",
      "|[CARACAS, (Reuters), -, Venezuelan, authorities, have, arrested, six, members, of, the, country’s...|\n",
      "|[\"If, only, every, day, could, be, like, this., You, can’t, put, your, finger, on, why:, Maybe, y...|\n",
      "|[Google, I/O,, the, company's, big, developer, conference,, kicks, off, today, (May, 18)., It's, ...|\n",
      "|[China, is, dismissing, unfavorable, media, reports, as, fake, news, because, that’s, what, Trump...|\n",
      "|[Elizabeth, Warren, is, giving, people, a, new, reason, to, pick, up, their, phones, when, a, cal...|\n",
      "|[(Reuters), -, The, success, of, Hudson’s, Bay, Co, Executive, Chairman, Richard, Baker’s, $1.3, ...|\n",
      "|[Joakim, Noah's, ﻿model, girlfriend, Lais, Ribeiro, was, so, damn, hot, in, her, thong, bikini, i...|\n",
      "|[\"Jermaine, Jackson, ﻿is, slamming, Quincy, Jones, for, removing, Michael, Jackson, references, f...|\n",
      "|[LONDON, (Reuters), -, British, Prime, Minister, Theresa, May, is, pressing, on, with, efforts, t...|\n",
      "|[\"Nancy, Pelosi, is, getting, visions, of, reclaiming, her, House, gavel., In, an, interview, pub...|\n",
      "|[The, nonpartisan, director, of, the, federal, Office, of, Government, Ethics, ripped, into, Pres...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_column.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_column.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text_column.select(explode(col(\"article\")).alias(\"tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|     tokens|\n",
      "+-----------+\n",
      "|    article|\n",
      "|      \"This|\n",
      "|       post|\n",
      "|         is|\n",
      "|       part|\n",
      "|         of|\n",
      "| Polyarchy,|\n",
      "|         an|\n",
      "|independent|\n",
      "|       blog|\n",
      "|   produced|\n",
      "|         by|\n",
      "|        the|\n",
      "|  political|\n",
      "|     reform|\n",
      "|    program|\n",
      "|         at|\n",
      "|        New|\n",
      "|   America,|\n",
      "|          a|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tokens = tokens.select(lower(col(\"tokens\")).alias(\"normalized tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|normalized tokens|\n",
      "+-----------------+\n",
      "|          article|\n",
      "|            \"this|\n",
      "|             post|\n",
      "|               is|\n",
      "|             part|\n",
      "|               of|\n",
      "|       polyarchy,|\n",
      "|               an|\n",
      "|      independent|\n",
      "|             blog|\n",
      "|         produced|\n",
      "|               by|\n",
      "|              the|\n",
      "|        political|\n",
      "|           reform|\n",
      "|          program|\n",
      "|               at|\n",
      "|              new|\n",
      "|         america,|\n",
      "|                a|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalized_tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = normalized_tokens.select(regexp_extract(col(\"normalized tokens\"),\"[a-z]*\",0).\n",
    "                                        alias(\"Cleaned Tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Cleaned Tokens|\n",
      "+--------------+\n",
      "|       article|\n",
      "|              |\n",
      "|          post|\n",
      "|            is|\n",
      "|          part|\n",
      "|            of|\n",
      "|     polyarchy|\n",
      "|            an|\n",
      "|   independent|\n",
      "|          blog|\n",
      "|      produced|\n",
      "|            by|\n",
      "|           the|\n",
      "|     political|\n",
      "|        reform|\n",
      "|       program|\n",
      "|            at|\n",
      "|           new|\n",
      "|       america|\n",
      "|             a|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_cleaned_tokens = clean_tokens.filter(col(\"Cleaned Tokens\") != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Cleaned Tokens|\n",
      "+--------------+\n",
      "|       article|\n",
      "|          post|\n",
      "|            is|\n",
      "|          part|\n",
      "|            of|\n",
      "|     polyarchy|\n",
      "|            an|\n",
      "|   independent|\n",
      "|          blog|\n",
      "|      produced|\n",
      "|            by|\n",
      "|           the|\n",
      "|     political|\n",
      "|        reform|\n",
      "|       program|\n",
      "|            at|\n",
      "|           new|\n",
      "|       america|\n",
      "|             a|\n",
      "|    washington|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "non_null_cleaned_tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_groups = non_null_cleaned_tokens.groupby(col(\"Cleaned Tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1ef6582d850>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = token_groups.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|Cleaned Tokens|  count|\n",
      "+--------------+-------+\n",
      "|          some|1348511|\n",
      "|    likelihood|  12071|\n",
      "|         still| 682599|\n",
      "|         those| 741766|\n",
      "|      tortured|   5220|\n",
      "|        online| 200524|\n",
      "|           few| 380259|\n",
      "|   transaction|  31312|\n",
      "|     indicator|   8984|\n",
      "|     involving|  42441|\n",
      "|      incoming|  12798|\n",
      "|     connected|  40889|\n",
      "|       jewelry|  15130|\n",
      "|        bazaar|   2039|\n",
      "|  safeguarding|   1675|\n",
      "|        filing|  58015|\n",
      "|        brands|  55095|\n",
      "|       flashed|   2282|\n",
      "|   interaction|   9840|\n",
      "|           art| 183475|\n",
      "+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|Cleaned Tokens|   count|\n",
      "+--------------+--------+\n",
      "|           the|49614097|\n",
      "|            to|25278753|\n",
      "|            of|22664966|\n",
      "|             a|22242784|\n",
      "|           and|21699171|\n",
      "|            in|18907734|\n",
      "|          that|10654902|\n",
      "|            on| 9087425|\n",
      "|           for| 9066347|\n",
      "|            is| 7811476|\n",
      "|            it| 7043139|\n",
      "|          with| 6416271|\n",
      "|            as| 5385847|\n",
      "|            by| 5363492|\n",
      "|           was| 5249367|\n",
      "|          said| 4944904|\n",
      "|            at| 4684935|\n",
      "|            he| 4379023|\n",
      "|          from| 4267604|\n",
      "|           are| 3800689|\n",
      "|            be| 3782725|\n",
      "|           has| 3751435|\n",
      "|          have| 3703778|\n",
      "|           but| 3560702|\n",
      "|            an| 3538650|\n",
      "|             i| 3406538|\n",
      "|          this| 3401500|\n",
      "|           his| 3371237|\n",
      "|           not| 3152063|\n",
      "|           you| 2872337|\n",
      "|          they| 2779378|\n",
      "|          more| 2692541|\n",
      "|           its| 2616173|\n",
      "|           who| 2578406|\n",
      "|          will| 2493764|\n",
      "|         their| 2374196|\n",
      "|         about| 2357556|\n",
      "|            we| 2339416|\n",
      "|            or| 2303646|\n",
      "|           new| 2250637|\n",
      "|           had| 2225702|\n",
      "|         which| 2127790|\n",
      "|           one| 2075520|\n",
      "|           her| 2072795|\n",
      "|         trump| 2016951|\n",
      "|           she| 2003116|\n",
      "|         would| 1986476|\n",
      "|            up| 1912074|\n",
      "|          been| 1869439|\n",
      "|          were| 1852990|\n",
      "|         after| 1849866|\n",
      "|           out| 1702758|\n",
      "|          also| 1697480|\n",
      "|           all| 1677483|\n",
      "|        people| 1673880|\n",
      "|           can| 1656043|\n",
      "|          when| 1647544|\n",
      "|          than| 1549684|\n",
      "|          like| 1531289|\n",
      "|          what| 1486246|\n",
      "|         there| 1485571|\n",
      "|            so| 1482132|\n",
      "|          year| 1480375|\n",
      "|            mr| 1424005|\n",
      "|            if| 1403218|\n",
      "|          over| 1388058|\n",
      "|         first| 1348724|\n",
      "|          some| 1348511|\n",
      "|           two| 1336832|\n",
      "|          into| 1311814|\n",
      "|       percent| 1281056|\n",
      "|         other| 1278639|\n",
      "|             u| 1276708|\n",
      "|          time| 1254426|\n",
      "|          last| 1237274|\n",
      "|     president| 1226881|\n",
      "|          just| 1224300|\n",
      "|       company| 1222047|\n",
      "|         could| 1173180|\n",
      "|            no| 1106479|\n",
      "|           our| 1066656|\n",
      "|           now| 1037935|\n",
      "|       million| 1037572|\n",
      "|          them| 1021216|\n",
      "|            my| 1016348|\n",
      "|         years| 1014736|\n",
      "|            do|  981478|\n",
      "|           how|  969898|\n",
      "|          your|  966904|\n",
      "|          most|  955236|\n",
      "|         while|  947594|\n",
      "|         state|  879958|\n",
      "|           get|  863950|\n",
      "|       because|  850873|\n",
      "|           may|  848894|\n",
      "|          even|  847753|\n",
      "|          only|  836419|\n",
      "|           him|  830422|\n",
      "|         world|  811494|\n",
      "|    government|  810561|\n",
      "+--------------+--------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_counts.orderBy(\"count\",ascending=False).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|                 _c0|                 _c1|                _c2|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|                null|          Unnamed: 0|               date|\n",
      "|                   0|                   0|2016-12-09 18:31:00|\n",
      "|                   1|                   1|2016-10-07 21:26:46|\n",
      "|The highest-paid ...|https://www.busin...|               null|\n",
      "|                   2|                   2|2018-01-26 00:00:00|\n",
      "|                   3|                   3|2019-06-27 00:00:00|\n",
      "|                   4|                   4|2016-01-27 00:00:00|\n",
      "|                   5|                   5|2019-06-17 00:00:00|\n",
      "|Editing by Tassil...|https://www.reute...|         Financials|\n",
      "|                   6|                   6|2019-06-23 00:00:00|\n",
      "|                   7|                   7|2018-05-02 17:09:00|\n",
      "|                   8|                   8|2016-05-18 13:00:06|\n",
      "|                   9|                   9|2017-03-02 00:00:00|\n",
      "|                  10|                  10|2019-05-22 20:10:00|\n",
      "|                  11|                  11|2019-06-23 00:00:00|\n",
      "|                  12|                  12|2018-11-05 00:00:00|\n",
      "|                  13|                  13|2019-05-10 00:00:00|\n",
      "|                  14|                  14|2019-03-28 00:00:00|\n",
      "|                  15|                  15|2016-09-08 15:40:02|\n",
      "|                  16|                  16|2017-01-11 23:10:01|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(*data.columns[:3]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|                 _c0|                 _c1|                _c2|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|                null|          Unnamed: 0|               date|\n",
      "|                   0|                   0|2016-12-09 18:31:00|\n",
      "|                   1|                   1|2016-10-07 21:26:46|\n",
      "|The highest-paid ...|https://www.busin...|               null|\n",
      "|                   2|                   2|2018-01-26 00:00:00|\n",
      "|                   3|                   3|2019-06-27 00:00:00|\n",
      "|                   4|                   4|2016-01-27 00:00:00|\n",
      "|                   5|                   5|2019-06-17 00:00:00|\n",
      "|Editing by Tassil...|https://www.reute...|         Financials|\n",
      "|                   6|                   6|2019-06-23 00:00:00|\n",
      "|                   7|                   7|2018-05-02 17:09:00|\n",
      "|                   8|                   8|2016-05-18 13:00:06|\n",
      "|                   9|                   9|2017-03-02 00:00:00|\n",
      "|                  10|                  10|2019-05-22 20:10:00|\n",
      "|                  11|                  11|2019-06-23 00:00:00|\n",
      "|                  12|                  12|2018-11-05 00:00:00|\n",
      "|                  13|                  13|2019-05-10 00:00:00|\n",
      "|                  14|                  14|2019-03-28 00:00:00|\n",
      "|                  15|                  15|2016-09-08 15:40:02|\n",
      "|                  16|                  16|2017-01-11 23:10:01|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(*[\"_c0\",\"_c1\",\"_c2\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|                 _c0|                 _c1|                _c2|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|                null|          Unnamed: 0|               date|\n",
      "|                   0|                   0|2016-12-09 18:31:00|\n",
      "|                   1|                   1|2016-10-07 21:26:46|\n",
      "|The highest-paid ...|https://www.busin...|               null|\n",
      "|                   2|                   2|2018-01-26 00:00:00|\n",
      "|                   3|                   3|2019-06-27 00:00:00|\n",
      "|                   4|                   4|2016-01-27 00:00:00|\n",
      "|                   5|                   5|2019-06-17 00:00:00|\n",
      "|Editing by Tassil...|https://www.reute...|         Financials|\n",
      "|                   6|                   6|2019-06-23 00:00:00|\n",
      "|                   7|                   7|2018-05-02 17:09:00|\n",
      "|                   8|                   8|2016-05-18 13:00:06|\n",
      "|                   9|                   9|2017-03-02 00:00:00|\n",
      "|                  10|                  10|2019-05-22 20:10:00|\n",
      "|                  11|                  11|2019-06-23 00:00:00|\n",
      "|                  12|                  12|2018-11-05 00:00:00|\n",
      "|                  13|                  13|2019-05-10 00:00:00|\n",
      "|                  14|                  14|2019-03-28 00:00:00|\n",
      "|                  15|                  15|2016-09-08 15:40:02|\n",
      "|                  16|                  16|2017-01-11 23:10:01|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"_c0\",\"_c1\",\"_c2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|                 _c0|                 _c1|                _c2|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|                null|          Unnamed: 0|               date|\n",
      "|                   0|                   0|2016-12-09 18:31:00|\n",
      "|                   1|                   1|2016-10-07 21:26:46|\n",
      "|The highest-paid ...|https://www.busin...|               null|\n",
      "|                   2|                   2|2018-01-26 00:00:00|\n",
      "|                   3|                   3|2019-06-27 00:00:00|\n",
      "|                   4|                   4|2016-01-27 00:00:00|\n",
      "|                   5|                   5|2019-06-17 00:00:00|\n",
      "|Editing by Tassil...|https://www.reute...|         Financials|\n",
      "|                   6|                   6|2019-06-23 00:00:00|\n",
      "|                   7|                   7|2018-05-02 17:09:00|\n",
      "|                   8|                   8|2016-05-18 13:00:06|\n",
      "|                   9|                   9|2017-03-02 00:00:00|\n",
      "|                  10|                  10|2019-05-22 20:10:00|\n",
      "|                  11|                  11|2019-06-23 00:00:00|\n",
      "|                  12|                  12|2018-11-05 00:00:00|\n",
      "|                  13|                  13|2019-05-10 00:00:00|\n",
      "|                  14|                  14|2019-03-28 00:00:00|\n",
      "|                  15|                  15|2016-09-08 15:40:02|\n",
      "|                  16|                  16|2017-01-11 23:10:01|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(col(\"_c0\"),col(\"_c1\"),col(\"_c2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|                 _c0|                 _c1|                _c2|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|                null|          Unnamed: 0|               date|\n",
      "|                   0|                   0|2016-12-09 18:31:00|\n",
      "|                   1|                   1|2016-10-07 21:26:46|\n",
      "|The highest-paid ...|https://www.busin...|               null|\n",
      "|                   2|                   2|2018-01-26 00:00:00|\n",
      "|                   3|                   3|2019-06-27 00:00:00|\n",
      "|                   4|                   4|2016-01-27 00:00:00|\n",
      "|                   5|                   5|2019-06-17 00:00:00|\n",
      "|Editing by Tassil...|https://www.reute...|         Financials|\n",
      "|                   6|                   6|2019-06-23 00:00:00|\n",
      "|                   7|                   7|2018-05-02 17:09:00|\n",
      "|                   8|                   8|2016-05-18 13:00:06|\n",
      "|                   9|                   9|2017-03-02 00:00:00|\n",
      "|                  10|                  10|2019-05-22 20:10:00|\n",
      "|                  11|                  11|2019-06-23 00:00:00|\n",
      "|                  12|                  12|2018-11-05 00:00:00|\n",
      "|                  13|                  13|2019-05-10 00:00:00|\n",
      "|                  14|                  14|2019-03-28 00:00:00|\n",
      "|                  15|                  15|2016-09-08 15:40:02|\n",
      "|                  16|                  16|2017-01-11 23:10:01|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(*[col(\"_c0\"),col(\"_c1\"),col(\"_c2\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " '_c1',\n",
       " '_c2',\n",
       " '_c3',\n",
       " '_c4',\n",
       " '_c5',\n",
       " '_c6',\n",
       " '_c7',\n",
       " '_c8',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_chunks = np.array_split(np.array(data.columns),len(data.columns)//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['_c0', '_c1', '_c2'], dtype='<U4'), array(['_c3', '_c4', '_c5'], dtype='<U4'), array(['_c6', '_c7', '_c8'], dtype='<U4'), array(['_c9', '_c10', '_c11'], dtype='<U4')]\n"
     ]
    }
   ],
   "source": [
    "print(column_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|                 _c0|                 _c1|                _c2|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|                null|          Unnamed: 0|               date|\n",
      "|                   0|                   0|2016-12-09 18:31:00|\n",
      "|                   1|                   1|2016-10-07 21:26:46|\n",
      "|The highest-paid ...|https://www.busin...|               null|\n",
      "|                   2|                   2|2018-01-26 00:00:00|\n",
      "|                   3|                   3|2019-06-27 00:00:00|\n",
      "|                   4|                   4|2016-01-27 00:00:00|\n",
      "|                   5|                   5|2019-06-17 00:00:00|\n",
      "|Editing by Tassil...|https://www.reute...|         Financials|\n",
      "|                   6|                   6|2019-06-23 00:00:00|\n",
      "|                   7|                   7|2018-05-02 17:09:00|\n",
      "|                   8|                   8|2016-05-18 13:00:06|\n",
      "|                   9|                   9|2017-03-02 00:00:00|\n",
      "|                  10|                  10|2019-05-22 20:10:00|\n",
      "|                  11|                  11|2019-06-23 00:00:00|\n",
      "|                  12|                  12|2018-11-05 00:00:00|\n",
      "|                  13|                  13|2019-05-10 00:00:00|\n",
      "|                  14|                  14|2019-03-28 00:00:00|\n",
      "|                  15|                  15|2016-09-08 15:40:02|\n",
      "|                  16|                  16|2017-01-11 23:10:01|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+-----+----+\n",
      "|             _c3|  _c4| _c5|\n",
      "+----------------+-----+----+\n",
      "|            year|month| day|\n",
      "|            2016| 12.0|   9|\n",
      "|            2016| 10.0|   7|\n",
      "|Business Insider| null|null|\n",
      "|            2018|  1.0|  26|\n",
      "|            2019|  6.0|  27|\n",
      "|            2016|  1.0|  27|\n",
      "|            2019|  6.0|  17|\n",
      "|         Reuters| null|null|\n",
      "|            2019|  6.0|  23|\n",
      "|            2018|  5.0|   2|\n",
      "|            2016|  5.0|  18|\n",
      "|            2017|  3.0|   2|\n",
      "|            2019|  5.0|  22|\n",
      "|            2019|  6.0|  23|\n",
      "|            2018| 11.0|   5|\n",
      "|            2019|  5.0|  10|\n",
      "|            2019|  3.0|  28|\n",
      "|            2016|  9.0|   8|\n",
      "|            2017|  1.0|  11|\n",
      "+----------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                 _c6|                 _c7|                 _c8|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|              author|               title|             article|\n",
      "|         Lee Drutman|We should take co...|\"This post is par...|\n",
      "|         Scott Davis|Colts GM Ryan Gri...| The Indianapolis...|\n",
      "|                null|                null|                null|\n",
      "|                null|Trump denies repo...|DAVOS, Switzerlan...|\n",
      "|                null|France's Sarkozy ...|PARIS (Reuters) -...|\n",
      "|                null|Paris Hilton: Wom...|\"Paris Hilton arr...|\n",
      "|                null|ECB's Coeure: If ...|BERLIN, June 17 (...|\n",
      "|                null|                null|                null|\n",
      "|                null|Venezuela detains...|CARACAS (Reuters)...|\n",
      "|   Caroline Williams|You Can Trick You...|\"If only every da...|\n",
      "|         Mark Bergen|How to watch the ...|Google I/O, the c...|\n",
      "|            Tim Hume|China is dismissi...|China is dismissi...|\n",
      "|       Emily Stewart|“Elizabeth Warren...|Elizabeth Warren ...|\n",
      "|Jessica DiNapoli,...|Hudson's Bay's ch...|(Reuters) - The s...|\n",
      "|                null|Joakim Noah's Vic...|Joakim Noah's ﻿mo...|\n",
      "|                null|Jermaine Jackson ...|\"Jermaine Jackson...|\n",
      "|                null|UK PM May presses...|LONDON (Reuters) ...|\n",
      "|          Jeff Stein|Nancy Pelosi says...|\"Nancy Pelosi is ...|\n",
      "|        Libby Nelson|The government of...|The nonpartisan d...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                 _c9|                _c10|                _c11|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                 url|             section|         publication|\n",
      "|             however| several critics ...|         for example|\n",
      "|                null|                null|                null|\n",
      "|                null|                null|                null|\n",
      "|https://www.reute...|               Davos|             Reuters|\n",
      "|https://www.reute...|          World News|             Reuters|\n",
      "|https://www.tmz.c...|                null|                 TMZ|\n",
      "|                null|                null|                null|\n",
      "|                null|                null|                null|\n",
      "|https://www.reute...|          World News|             Reuters|\n",
      "|\"\" DeGutis says. ...| paying attention...| it makes it easi...|\n",
      "|https://www.vox.c...|                null|                 Vox|\n",
      "|https://news.vice...|                null|           Vice News|\n",
      "|https://www.vox.c...|                null|                 Vox|\n",
      "|https://www.reute...|       Business News|             Reuters|\n",
      "|https://www.tmz.c...|                null|                 TMZ|\n",
      "|\"\" \"\"Thriller\"\" a...| and the 2 will f...| Quincy isn't say...|\n",
      "|https://www.reute...|          World News|             Reuters|\n",
      "|\"\" Skelley says. ...|           of course| but it begins lo...|\n",
      "|https://www.vox.c...|                null|                 Vox|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in column_chunks:\n",
    "    data.select(*x).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_data = data.drop(*data.columns[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 _c6|                 _c7|                 _c8|                 _c9|                _c10|                _c11|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              author|               title|             article|                 url|             section|         publication|\n",
      "|         Lee Drutman|We should take co...|\"This post is par...|             however| several critics ...|         for example|\n",
      "|         Scott Davis|Colts GM Ryan Gri...| The Indianapolis...|                null|                null|                null|\n",
      "|                null|                null|                null|                null|                null|                null|\n",
      "|                null|Trump denies repo...|DAVOS, Switzerlan...|https://www.reute...|               Davos|             Reuters|\n",
      "|                null|France's Sarkozy ...|PARIS (Reuters) -...|https://www.reute...|          World News|             Reuters|\n",
      "|                null|Paris Hilton: Wom...|\"Paris Hilton arr...|https://www.tmz.c...|                null|                 TMZ|\n",
      "|                null|ECB's Coeure: If ...|BERLIN, June 17 (...|                null|                null|                null|\n",
      "|                null|                null|                null|                null|                null|                null|\n",
      "|                null|Venezuela detains...|CARACAS (Reuters)...|https://www.reute...|          World News|             Reuters|\n",
      "|   Caroline Williams|You Can Trick You...|\"If only every da...|\"\" DeGutis says. ...| paying attention...| it makes it easi...|\n",
      "|         Mark Bergen|How to watch the ...|Google I/O, the c...|https://www.vox.c...|                null|                 Vox|\n",
      "|            Tim Hume|China is dismissi...|China is dismissi...|https://news.vice...|                null|           Vice News|\n",
      "|       Emily Stewart|“Elizabeth Warren...|Elizabeth Warren ...|https://www.vox.c...|                null|                 Vox|\n",
      "|Jessica DiNapoli,...|Hudson's Bay's ch...|(Reuters) - The s...|https://www.reute...|       Business News|             Reuters|\n",
      "|                null|Joakim Noah's Vic...|Joakim Noah's ﻿mo...|https://www.tmz.c...|                null|                 TMZ|\n",
      "|                null|Jermaine Jackson ...|\"Jermaine Jackson...|\"\" \"\"Thriller\"\" a...| and the 2 will f...| Quincy isn't say...|\n",
      "|                null|UK PM May presses...|LONDON (Reuters) ...|https://www.reute...|          World News|             Reuters|\n",
      "|          Jeff Stein|Nancy Pelosi says...|\"Nancy Pelosi is ...|\"\" Skelley says. ...|           of course| but it begins lo...|\n",
      "|        Libby Nelson|The government of...|The nonpartisan d...|https://www.vox.c...|                null|                 Vox|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_data = dropped_data.drop(col(\"_c9\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 _c6|                 _c7|                 _c8|                _c10|                _c11|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              author|               title|             article|             section|         publication|\n",
      "|         Lee Drutman|We should take co...|\"This post is par...| several critics ...|         for example|\n",
      "|         Scott Davis|Colts GM Ryan Gri...| The Indianapolis...|                null|                null|\n",
      "|                null|                null|                null|                null|                null|\n",
      "|                null|Trump denies repo...|DAVOS, Switzerlan...|               Davos|             Reuters|\n",
      "|                null|France's Sarkozy ...|PARIS (Reuters) -...|          World News|             Reuters|\n",
      "|                null|Paris Hilton: Wom...|\"Paris Hilton arr...|                null|                 TMZ|\n",
      "|                null|ECB's Coeure: If ...|BERLIN, June 17 (...|                null|                null|\n",
      "|                null|                null|                null|                null|                null|\n",
      "|                null|Venezuela detains...|CARACAS (Reuters)...|          World News|             Reuters|\n",
      "|   Caroline Williams|You Can Trick You...|\"If only every da...| paying attention...| it makes it easi...|\n",
      "|         Mark Bergen|How to watch the ...|Google I/O, the c...|                null|                 Vox|\n",
      "|            Tim Hume|China is dismissi...|China is dismissi...|                null|           Vice News|\n",
      "|       Emily Stewart|“Elizabeth Warren...|Elizabeth Warren ...|                null|                 Vox|\n",
      "|Jessica DiNapoli,...|Hudson's Bay's ch...|(Reuters) - The s...|       Business News|             Reuters|\n",
      "|                null|Joakim Noah's Vic...|Joakim Noah's ﻿mo...|                null|                 TMZ|\n",
      "|                null|Jermaine Jackson ...|\"Jermaine Jackson...| and the 2 will f...| Quincy isn't say...|\n",
      "|                null|UK PM May presses...|LONDON (Reuters) ...|          World News|             Reuters|\n",
      "|          Jeff Stein|Nancy Pelosi says...|\"Nancy Pelosi is ...|           of course| but it begins lo...|\n",
      "|        Libby Nelson|The government of...|The nonpartisan d...|                null|                 Vox|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_dropped_data = data.select(\n",
    "                        *[x for x in data.columns if x in [\"_c6\",\"_c7\",\"_c8\",\"_c10\",\"_c11\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 _c6|                 _c7|                 _c8|                _c10|                _c11|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              author|               title|             article|             section|         publication|\n",
      "|         Lee Drutman|We should take co...|\"This post is par...| several critics ...|         for example|\n",
      "|         Scott Davis|Colts GM Ryan Gri...| The Indianapolis...|                null|                null|\n",
      "|                null|                null|                null|                null|                null|\n",
      "|                null|Trump denies repo...|DAVOS, Switzerlan...|               Davos|             Reuters|\n",
      "|                null|France's Sarkozy ...|PARIS (Reuters) -...|          World News|             Reuters|\n",
      "|                null|Paris Hilton: Wom...|\"Paris Hilton arr...|                null|                 TMZ|\n",
      "|                null|ECB's Coeure: If ...|BERLIN, June 17 (...|                null|                null|\n",
      "|                null|                null|                null|                null|                null|\n",
      "|                null|Venezuela detains...|CARACAS (Reuters)...|          World News|             Reuters|\n",
      "|   Caroline Williams|You Can Trick You...|\"If only every da...| paying attention...| it makes it easi...|\n",
      "|         Mark Bergen|How to watch the ...|Google I/O, the c...|                null|                 Vox|\n",
      "|            Tim Hume|China is dismissi...|China is dismissi...|                null|           Vice News|\n",
      "|       Emily Stewart|“Elizabeth Warren...|Elizabeth Warren ...|                null|                 Vox|\n",
      "|Jessica DiNapoli,...|Hudson's Bay's ch...|(Reuters) - The s...|       Business News|             Reuters|\n",
      "|                null|Joakim Noah's Vic...|Joakim Noah's ﻿mo...|                null|                 TMZ|\n",
      "|                null|Jermaine Jackson ...|\"Jermaine Jackson...| and the 2 will f...| Quincy isn't say...|\n",
      "|                null|UK PM May presses...|LONDON (Reuters) ...|          World News|             Reuters|\n",
      "|          Jeff Stein|Nancy Pelosi says...|\"Nancy Pelosi is ...|           of course| but it begins lo...|\n",
      "|        Libby Nelson|The government of...|The nonpartisan d...|                null|                 Vox|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "another_dropped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_dropped_data = another_dropped_data.withColumnRenamed(\"_c6\",\"author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              author|                 _c7|                 _c8|                _c10|                _c11|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              author|               title|             article|             section|         publication|\n",
      "|         Lee Drutman|We should take co...|\"This post is par...| several critics ...|         for example|\n",
      "|         Scott Davis|Colts GM Ryan Gri...| The Indianapolis...|                null|                null|\n",
      "|                null|                null|                null|                null|                null|\n",
      "|                null|Trump denies repo...|DAVOS, Switzerlan...|               Davos|             Reuters|\n",
      "|                null|France's Sarkozy ...|PARIS (Reuters) -...|          World News|             Reuters|\n",
      "|                null|Paris Hilton: Wom...|\"Paris Hilton arr...|                null|                 TMZ|\n",
      "|                null|ECB's Coeure: If ...|BERLIN, June 17 (...|                null|                null|\n",
      "|                null|                null|                null|                null|                null|\n",
      "|                null|Venezuela detains...|CARACAS (Reuters)...|          World News|             Reuters|\n",
      "|   Caroline Williams|You Can Trick You...|\"If only every da...| paying attention...| it makes it easi...|\n",
      "|         Mark Bergen|How to watch the ...|Google I/O, the c...|                null|                 Vox|\n",
      "|            Tim Hume|China is dismissi...|China is dismissi...|                null|           Vice News|\n",
      "|       Emily Stewart|“Elizabeth Warren...|Elizabeth Warren ...|                null|                 Vox|\n",
      "|Jessica DiNapoli,...|Hudson's Bay's ch...|(Reuters) - The s...|       Business News|             Reuters|\n",
      "|                null|Joakim Noah's Vic...|Joakim Noah's ﻿mo...|                null|                 TMZ|\n",
      "|                null|Jermaine Jackson ...|\"Jermaine Jackson...| and the 2 will f...| Quincy isn't say...|\n",
      "|                null|UK PM May presses...|LONDON (Reuters) ...|          World News|             Reuters|\n",
      "|          Jeff Stein|Nancy Pelosi says...|\"Nancy Pelosi is ...|           of course| but it begins lo...|\n",
      "|        Libby Nelson|The government of...|The nonpartisan d...|                null|                 Vox|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "another_dropped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_random_collection = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conftfos = pyspark.SparkConf().setAll([('spark.executor.cores','1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Converting articles into BoW Vectors, master=local[*]) created by getOrCreate at <ipython-input-2-e2daf0041fd5>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-185c939fdeab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[*]\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconftfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                     \u001b[1;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    332\u001b[0m                         \u001b[1;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m                         \u001b[1;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Converting articles into BoW Vectors, master=local[*]) created by getOrCreate at <ipython-input-2-e2daf0041fd5>:1 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local[*]\",conf=conftfos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_collection_rdd = sc.parallelize(some_random_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "print(random_collection_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one(element):\n",
    "    return element + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_result = random_collection_rdd.map(add_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (DESKTOP-MLVSQCI executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 586, in main\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from 'C:\\\\Users\\\\aimlrl-amd ryzen\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\pyspark.zip\\\\pyspark\\\\cloudpickle\\\\__init__.py'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 586, in main\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from 'C:\\\\Users\\\\aimlrl-amd ryzen\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\pyspark.zip\\\\pyspark\\\\cloudpickle\\\\__init__.py'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-88278aa1307d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfunc_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\pyspark\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\pyspark\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (DESKTOP-MLVSQCI executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 586, in main\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from 'C:\\\\Users\\\\aimlrl-amd ryzen\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\pyspark.zip\\\\pyspark\\\\cloudpickle\\\\__init__.py'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 586, in main\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Users\\aimlrl-amd ryzen\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from 'C:\\\\Users\\\\aimlrl-amd ryzen\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\lib\\\\pyspark.zip\\\\pyspark\\\\cloudpickle\\\\__init__.py'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "func_result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
